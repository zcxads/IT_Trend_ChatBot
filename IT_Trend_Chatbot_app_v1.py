import streamlit as st
import os

from langchain_core.messages import HumanMessage, AIMessage, ChatMessage
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_community.chat_message_histories import ChatMessageHistory



########################################## ê²½ë¡œ ì§€ì • ##################################################
VECTOR_STORE_PATH = "/IT_trend_chatbot/faiss_db"
CACHE_DIR = ".cache"
EMBEDDINGS_DIR = ".cache/embeddings"

# Sidebar
with st.sidebar:
    clear_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")
    selected_model = st.selectbox("LLM ì„ íƒ", ["gemma2:2b", "gemma2"], index=0)
    session_id = st.text_input("ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.", "abc123")

# ìºì‹œ & ì„ë² ë”© ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(EMBEDDINGS_DIR, exist_ok=True)

################################### ì„¸ì…˜ ì´ˆê¸°í™” ######################################
# session_state : ì›¹ ì•±ì˜ ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ê´€ë¦¬í•˜ëŠ” ë°©ë²• = ì‚¬ìš©ì ì„¸ì…˜ ë™ì•ˆ ë°ì´í„°ë¥¼ ìœ ì§€
    # dictì™€ ìœ ì‚¬í•˜ê²Œ ì‘ë™, í‚¤-ê°’ ìŒ ì €ì¥
    # streamlitì€ ê¸°ë³¸ì ìœ¼ë¡œ ì‹¤í–‰í•  ë•Œ ë§ˆë‹¤, ì „ì²´ í˜ì´ì§€ ë‹¤ì‹œ ì‹¤í–‰í•˜ëŠ”ë° ì„¸ì…˜ì„ í†µí•´ ë°ì´í„° ìœ ì§€ê°€ëŠ¥

# messages : ì‚¬ìš©ìì™€ ì±—ë´‡ ê°„ì˜ ëŒ€í™” ê¸°ë¡
if "messages" not in st.session_state:
    st.session_state.messages = []
# RAG chain -> ì´í›„ create_rag_chain() í•¨ìˆ˜ë¥¼ í†µí•´ ì‹¤ì œ chainê°ì²´ë¡œ ì´ˆê¸°í™”
if "chain" not in st.session_state:
    st.session_state.chain = None
# store : ì—¬ëŸ¬ ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” dict
    # ê° ì„¸ì…˜ IDë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ ì„¸ì…˜ì˜ ChatMessageHistoryê°ì²´ ì €ì¥ -> ì—¬ëŸ¬ ì‚¬ìš©ì(ëŒ€í™”) ì„¸ì…˜ ê´€ë¦¬
if "store" not in st.session_state:
    st.session_state.store = {}


# ê° ì„¸ì…˜ì— ëŒ€í•œ ëŒ€í™”ê¸°ë¡ ê´€ë¦¬
def get_session_history(session_id):
    if session_id not in st.session_state.store:
        st.session_state.store[session_id] = ChatMessageHistory()
    return st.session_state.store[session_id]

################################# ì„ë² ë”© ì •ì˜ ###########################################

@st.cache_resource
def get_embeddings():
    return HuggingFaceBgeEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"}, # cuda or cpu
        encode_kwargs={"normalize_embeddings": True}
    )

# ì„ë² ë”©ì´ ë‹¤ì–‘í•œ ì…ë ¥ í˜•ì‹ì„ ì²˜ë¦¬í•˜ë„ë¡ í•¨ : ì„ë² ë”©ì´ í•­ìƒ ë¬¸ìì—´ ì…ë ¥ì„ ë°›ë„ë¡ ë³´ì¥
def safe_embed_query(embedding_function, text):
    if isinstance(text, dict) and "question" in text:
        text = text["question"]
    if not isinstance(text, str):
        text = str(text)
    return embedding_function.embed_query(text)

############################### ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ############################################

@st.cache_resource(show_spinner="ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ... or ìƒì„±...ì¤‘ì…ë‹ˆë‹¤.")
def load_or_create_vector_store():
    embeddings = get_embeddings()
    
    if os.path.exists(VECTOR_STORE_PATH):
        vectorstore = FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    else:
        pass
    
    vectorstore._embed_query = lambda text: safe_embed_query(embeddings, text)
    
    return vectorstore

################################################### ì²´ì¸ìƒì„± ####################################################

def create_rag_chain():
    vectorstore = load_or_create_vector_store()
    retriever = vectorstore.as_retriever()
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", """
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

1. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
4. ë‹µë³€ì€ 3-4ë¬¸ì¥ì„ ë„˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:
""")
    ])
    
    llm = ChatOllama(model=selected_model, temperature=0)
    
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)
    
    chain = (
        RunnablePassthrough.assign(
            context=retriever | format_docs
        )
        | prompt
        | llm
        | StrOutputParser()
    )
    # RunnableWithMessageHistory : ê¸°ì¡´ chainì— ëŒ€í™”ê¸°ë¡ê´€ë¦¬ ê¸°ëŠ¥ ì¶”ê°€
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )
    
    return chain_with_history

############################################ ìŠ¤íŠ¸ë¦¼ë¦¿ ##############################################

# Streamlit UI
st.title("IT íŠ¸ë Œë“œ ì±—ë´‡ ğŸ’¬")



# chain ê°ì²´ ìƒì„±
if st.session_state.chain is None:
    st.session_state.chain = create_rag_chain()

# ëŒ€í™”ê¸°ë¡ ì´ˆê¸°í™” ë° ìƒˆë¡œìš´ ì„¸ì…˜ í• ë‹¹
if clear_btn:
    st.session_state.messages = []
    st.session_state.store[session_id] = ChatMessageHistory()

# ëŒ€í™”ê¸°ë¡ ui ìƒì„±
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input : ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
if user_input := st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!"):
    st.session_state.messages.append({"role": "user", "content": user_input}) # ëŒ€í™”ê¸°ë¡ ì¶”ê°€
    with st.chat_message("user"): # streamlitì— ì‚¬ìš©ì(user) ë©”ì„¸ì§€ í‘œì‹œ
        st.markdown(user_input)

    with st.chat_message("assistant"): # streamlitì— ì±—ë´‡(assistant) ë©”ì„¸ì§€ í‘œì‹œ
        message_placeholder = st.empty() # ì‘ë‹µì´ ìƒì„±ë˜ëŠ” ë™ì•ˆ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ë  placeholderìƒì„±
        full_response = ""
        
        # Stream the response
        try:
            for chunk in st.session_state.chain.stream(
                {"question": user_input},
                config={"configurable": {"session_id": session_id}}
            ):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")
        except Exception as e:
            st.error(f"An error occurred: {str(e)}")
            full_response = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response}) # ëŒ€í™”ê¸°ë¡ ì¶”ê°€

if st.button("Clear Chat History"):
    st.session_state.messages = []
    st.session_state.store[session_id] = ChatMessageHistory()
    st.rerun()